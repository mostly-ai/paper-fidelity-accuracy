{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swBPCgIhaX9H"
   },
   "source": [
    "# Synthetic Data Evaluation 2023-05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8VPgEvYf-Le"
   },
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yYeyS8P7f9U0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm import early_stopping\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p2faHMNwpZXF"
   },
   "source": [
    "## Define Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u6N7O40FpZXF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "repo = './data/'\n",
    "#repo = 'https://github.com/mostly-ai/paper-fidelity-accuracy/raw/main/data/2023-05/'\n",
    "\n",
    "# list of datasets to benchmark\n",
    "datasets = [\n",
    "    'adult',\n",
    "    'credit-default', \n",
    "    'bank-marketing', \n",
    "    'online-shoppers'\n",
    "]\n",
    "\n",
    "# list of synthesizers to benchmark\n",
    "fns = [\n",
    "    'mostly', \n",
    "    # 'copulagan', \n",
    "    # 'ctgan', \n",
    "    # 'tvae', \n",
    "    # 'gaussian_copula', \n",
    "    'synthpop',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XC5d75T4wppU"
   },
   "source": [
    "# Fidelity Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r_DL7pGuwwSL",
    "outputId": "a0ca4243-0790-4b81-ce5c-9027a85b8295",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bin_data(dt1, dt2, c = 10):\n",
    "    dt1 = dt1.copy()\n",
    "    dt2 = dt2.copy()\n",
    "    # quantile binning of numerics\n",
    "    num_cols = dt1.select_dtypes(include='number').columns\n",
    "    for col in num_cols:\n",
    "        # determine breaks based on `dt1`\n",
    "        breaks = dt1[col].quantile(np.linspace(0, 1, c+1)).unique()\n",
    "        dt1[col] = pd.cut(dt1[col], bins=breaks, include_lowest=True).astype(str)\n",
    "        dt2_vals = pd.to_numeric(dt2[col], 'coerce')\n",
    "        dt2_bins = pd.cut(dt2_vals, bins=breaks, include_lowest=True).astype(str)\n",
    "        dt2_bins[dt2_vals < min(breaks)] = '_other_'\n",
    "        dt2_bins[dt2_vals > max(breaks)] = '_other_'\n",
    "        dt2[col] = dt2_bins\n",
    "    # # convert bools to categoricals\n",
    "    bool_cols = dt1.select_dtypes(include=['bool'])\n",
    "    for col in bool_cols:\n",
    "        dt1[col] = dt1[col].astype('str')\n",
    "        dt2[col] = dt2[col].astype('str')\n",
    "    # top-C binning of categoricals\n",
    "    cat_cols = dt1.select_dtypes(include=['object', 'category', 'string'])\n",
    "    for col in cat_cols:\n",
    "        # determine top values based on `dt1`\n",
    "        top_vals = dt1[col].value_counts().head(c).index.tolist()\n",
    "        dt1[col].replace(np.setdiff1d(dt1[col].unique().tolist(), top_vals), '_other_', inplace=True)\n",
    "        dt2[col].replace(np.setdiff1d(dt2[col].unique().tolist(), top_vals), '_other_', inplace=True)\n",
    "    return [dt1, dt2]\n",
    "\n",
    "def hellinger(p1, p2):\n",
    "  return np.sqrt(1 - np.sum(np.sqrt(p1*p2)))\n",
    "\n",
    "def kullback_leibler(p1, p2):\n",
    "  idx = p1>0\n",
    "  return np.sum(p1[idx] * np.log(p1[idx]/p2[idx]))\n",
    "\n",
    "def jensen_shannon(p1, p2):\n",
    "  m = 0.5 * (p1 + p2)\n",
    "  return 0.5 * kullback_leibler(p1, m) + 0.5 * kullback_leibler(p2, m)\n",
    "\n",
    "def fidelity(dt1, dt2, c = 100, k = 1):\n",
    "    [dt1_bin, dt2_bin] = bin_data(dt1, dt2, c = c)\n",
    "    # build grid of all cross-combinations\n",
    "    cols = trn.columns\n",
    "    interactions = pd.DataFrame(np.array(np.meshgrid(cols, cols, cols)).reshape(3, len(cols)**3).T)\n",
    "    interactions.columns = ['dim1', 'dim2', 'dim3']\n",
    "    if k == 1:\n",
    "        interactions = interactions.loc[(interactions['dim1']==interactions['dim2']) & (interactions['dim2']==interactions['dim3'])]\n",
    "    elif k == 2:\n",
    "        interactions = interactions.loc[(interactions['dim1']<interactions['dim2']) & (interactions['dim2']==interactions['dim3'])]\n",
    "    elif k == 3:\n",
    "        interactions = interactions.loc[(interactions['dim1']<interactions['dim2']) & (interactions['dim2']<interactions['dim3'])]\n",
    "        # interactions = interactions.sample(n=min(1_000, len(interactions)))\n",
    "    else:\n",
    "        raise('k>3 not supported')\n",
    "\n",
    "    results = []\n",
    "    for idx in range(interactions.shape[0]):\n",
    "        row = interactions.iloc[idx]\n",
    "        val1 = dt1_bin[row.dim1] + dt1_bin[row.dim2] + dt1_bin[row.dim3]\n",
    "        val2 = dt2_bin[row.dim1] + dt2_bin[row.dim2] + dt2_bin[row.dim3]\n",
    "        freq1 = val1.value_counts(normalize=True).to_frame(name='p1')\n",
    "        freq2 = val2.value_counts(normalize=True).to_frame(name='p2')\n",
    "        freq = freq1.join(freq2, how='outer').fillna(0.0)\n",
    "        p1 = freq['p1']\n",
    "        p2 = freq['p2']\n",
    "        out = pd.DataFrame({\n",
    "          'k': f\"{k=}\",\n",
    "          'dim1': [row.dim1], \n",
    "          'dim2': [row.dim2], \n",
    "          'dim3': [row.dim3],\n",
    "          'tvd': [np.sum(np.abs(p1 - p2)) / 2], \n",
    "          'mae': [np.mean(np.abs(p1 - p2))], \n",
    "          'max': [np.max(np.abs(p1 - p2))],\n",
    "          'l1d': [np.sum(np.abs(p1 - p2))],\n",
    "          'l2d': [np.sqrt(np.sum((p1 - p2)**2))],\n",
    "          'hellinger': [hellinger(p1, p2)],\n",
    "          'jensen_shannon': [jensen_shannon(p1, p2)]\n",
    "        })\n",
    "        results.append(out)\n",
    "\n",
    "    return pd.concat(results)\n",
    "\n",
    "fid = []\n",
    "for dataset in datasets:\n",
    "    trn_fn = f\"{repo}{dataset}_trn.csv.gz\"\n",
    "    trn = pd.read_csv(trn_fn)\n",
    "    print(f\"read {trn_fn} {trn.shape}\")\n",
    "    for fn in fns + ['val']:\n",
    "        syn_fn = f\"{repo}{dataset}_{fn}.csv.gz\"\n",
    "        syn = pd.read_csv(syn_fn)\n",
    "        print(f\"read {syn_fn} {syn.shape}\")\n",
    "        # 1-way marginal distributions (=univariates)\n",
    "        fid1 = fidelity(trn, syn, k=1, c=100)\n",
    "        print('calculate 1-way fidelity')\n",
    "        # 2-way marginal distributions (=bivariates)\n",
    "        fid2 = fidelity(trn, syn, k=2, c=10)\n",
    "        print('calculate 2-way fidelity')\n",
    "        # 3-way marginal distributions; these are compute-intensive\n",
    "        fid3 = fidelity(trn, syn, k=3, c=5)\n",
    "        print('calculate 3-way fidelity')\n",
    "        out = pd.concat([fid1, fid2, fid3])\n",
    "        out['dataset'] = dataset\n",
    "        out['synthesizer'] = fn if fn!='val' else 'HOLDOUT'\n",
    "        fid.append(out)\n",
    "\n",
    "fid = pd.concat(fid)\n",
    "fid.to_csv('fidelity.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "id": "UL2dgeK_yZX2",
    "outputId": "818dcdc0-2d85-4935-be67-121f322243aa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "o219QXU1wwxC",
    "outputId": "7917e20a-798f-4439-ac3f-43c71e5a0b51",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate average \n",
    "agg = fid.groupby(['dataset', 'synthesizer', 'k'])['tvd'].mean().to_frame().reset_index()\n",
    "# convert to wide format\n",
    "agg = agg.pivot_table(index=['dataset', 'synthesizer'], columns=['k'], values='tvd').reset_index(['dataset', 'synthesizer'])\n",
    "agg = agg.sort_values(['dataset', 'k=1']).reset_index(drop=True)\n",
    "agg.columns.name = None\n",
    "agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IQUSuh3QpZXL",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot(agg, dataset, metric, title):\n",
    "    d = agg.loc[agg['dataset']==dataset]\n",
    "    p = d.plot(kind='barh', x='synthesizer', y=metric, title=f\"{title} - {dataset}\")\n",
    "    p.axvline(d.loc[d.synthesizer=='HOLDOUT', metric].to_numpy()[0], color='black')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "JqD__zhmpZXL",
    "outputId": "41370cb6-c720-4764-e2bd-ba2bbbf4b384",
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot(agg, 'adult', 'k=1', 'Univariate TVD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "UiidV88_pZXM",
    "outputId": "15f7628d-83b6-456f-b7d0-3d0a1de26702",
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot(agg, 'adult', 'k=2', 'Bivariate TVD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eCu5sIgx0V2N",
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot(agg, 'adult', 'k=3', 'Three-way TVD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BlaPPoWwYLO"
   },
   "source": [
    "# Machine Learning Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CQ3D5K-KpZXO",
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = 'adult'\n",
    "\n",
    "if dataset == 'adult':\n",
    "    target_col = 'income'\n",
    "    target_val = '>50K'\n",
    "elif dataset == 'bank-marketing':\n",
    "    target_col = 'y'\n",
    "    target_val = 'yes'\n",
    "elif dataset == 'credit-default':\n",
    "    target_col = 'default payment next month'\n",
    "    target_val = 1\n",
    "elif dataset == 'online-shoppers':\n",
    "    target_col = 'Revenue'\n",
    "    target_val = 'True'\n",
    "\n",
    "def prepare_xy(df):\n",
    "    y = (df[target_col]==target_val).astype(int)\n",
    "    str_cols = [col for col in df.select_dtypes(['object', 'string']).columns if col != target_col]\n",
    "    for col in str_cols:\n",
    "        df[col] = pd.Categorical(df[col])\n",
    "    cat_cols = [col for col in df.select_dtypes('category').columns if col != target_col]\n",
    "    num_cols = [col for col in df.select_dtypes('number').columns if col != target_col]\n",
    "    for col in num_cols:\n",
    "        df[col] = df[col].astype('float')\n",
    "    X = df[cat_cols + num_cols]\n",
    "    return X, y\n",
    "\n",
    "def train_model(X, y):\n",
    "    cat_cols = list(X.select_dtypes('category').columns)\n",
    "    X_trn, X_val, y_trn, y_val = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "    ds_trn = lgb.Dataset(X_trn, label=y_trn, categorical_feature=cat_cols, free_raw_data=False)\n",
    "    ds_val = lgb.Dataset(X_val, label=y_val, categorical_feature=cat_cols, free_raw_data=False)\n",
    "    model = lgb.train(\n",
    "        params={\n",
    "            'verbose': -1,\n",
    "            'metric': 'auc',  \n",
    "            'objective': 'binary'\n",
    "        }, \n",
    "        train_set=ds_trn,\n",
    "        valid_sets=[ds_val],\n",
    "        callbacks=[early_stopping(5)],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, hol):\n",
    "    X_hol, y_hol = prepare_xy(hol)\n",
    "    probs = model.predict(X_hol)\n",
    "    preds = (probs >= 0.5).astype(int)\n",
    "    auc = roc_auc_score(y_hol, probs)\n",
    "    acc = accuracy_score(y_hol, preds)\n",
    "    out = pd.DataFrame({\n",
    "          'auc': [auc],\n",
    "          'acc': [acc], \n",
    "        })\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0RxezjXipZXP",
    "outputId": "7b9f4d17-56c6-434d-8655-e8252e4dcfd5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_fn = f\"{repo}{dataset}_val.csv.gz\"\n",
    "val = pd.read_csv(val_fn)\n",
    "print(f\"read {val_fn} {val.shape}\")\n",
    "results = []\n",
    "for fn in fns + ['trn']:\n",
    "    syn_fn = f\"{repo}{dataset}_{fn}.csv.gz\"\n",
    "    syn = pd.read_csv(syn_fn)\n",
    "    print(f\"read {syn_fn} {syn.shape}\")\n",
    "    X, y = prepare_xy(syn)\n",
    "    m = train_model(X, y)\n",
    "    row = evaluate_model(m, val)\n",
    "    row = row.assign(synthesizer=fn if fn!='trn' else 'ORIGINAL')\n",
    "    row = row.assign(dataset=dataset)\n",
    "    results.append(row)\n",
    "\n",
    "ml = pd.concat(results).sort_values('auc', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "UzTxvw3hpZXQ",
    "outputId": "520ad0d1-a364-42e6-9031-c4b14a47fe4c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "niAEjJqDpZXQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot(agg, dataset, metric, title):\n",
    "    d = agg.loc[agg['dataset']==dataset]\n",
    "    p = d.plot(kind='scatter', x='synthesizer', y=metric, title=f\"{title} - {dataset}\", s=100)\n",
    "    p.axhline(d.loc[d.synthesizer=='ORIGINAL', metric].to_numpy()[0], color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "K75VDi3gpZXQ",
    "outputId": "062654d1-beed-402d-e991-805e62070cbf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot(ml, 'adult', 'auc', 'ML Performance - Holdout AUC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "-MwtvG_YpZXR",
    "outputId": "d2fb707e-7618-4e33-c024-046cc0e831ba",
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot(ml, 'adult', 'acc', 'ML Performance - Holdout Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JE5MgaQBwxpn"
   },
   "source": [
    "# Privacy Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "9dfREpZc3LWY",
    "outputId": "5be52fd5-5804-4850-b759-f68d7a442a07",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def priv(trn, val, syn, is_val, c = 100):\n",
    "    n_trn = trn.shape[0]\n",
    "    n_val = val.shape[0]\n",
    "    max_syn = 2_000  # cap to keep compute reasonable\n",
    "    n_syn = min(max_syn, syn.shape[0])\n",
    "    syn = syn[:n_syn]\n",
    "    print(f\"{n_trn=}, {n_val=}, {n_syn=}\")\n",
    "    numeric_cols = trn.select_dtypes(include=np.number).columns\n",
    "    other_cols = trn.select_dtypes(exclude=np.number).columns\n",
    "    transformer = make_column_transformer(\n",
    "        (OneHotEncoder(), other_cols),\n",
    "        (QuantileTransformer(output_distribution='normal'), numeric_cols),\n",
    "        remainder=\"passthrough\",\n",
    "    )\n",
    "    transformer.fit(pd.concat([trn, val, syn], axis=0))\n",
    "    trn_hot = transformer.transform(trn)\n",
    "    val_hot = transformer.transform(val)\n",
    "    syn_hot = transformer.transform(syn)\n",
    "    # metrics: https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html\n",
    "    print('calculate distances to training data')\n",
    "    index = NearestNeighbors(n_neighbors=2, algorithm=\"brute\", metric=\"l2\", n_jobs=-1)\n",
    "    index.fit(trn_hot)\n",
    "    dcrs_trn, _ = index.kneighbors(syn_hot)\n",
    "    print('calculate distances to holdout data')\n",
    "    index = NearestNeighbors(n_neighbors=2, algorithm=\"brute\", metric=\"l2\", n_jobs=-1)\n",
    "    index.fit(val_hot)\n",
    "    dcrs_val, _ = index.kneighbors(syn_hot)\n",
    "    dists_trn = np.square(dcrs_trn)[:,0]/2\n",
    "    dists_val = np.square(dcrs_val)[:,0]/2\n",
    "    # results\n",
    "    share = np.mean(dists_trn<dists_val) + (n_trn/(n_trn+n_val)) * np.mean(dists_trn==dists_val)\n",
    "    out = pd.DataFrame({\n",
    "        'n_syn': [n_syn],\n",
    "        'n_closer': np.sum([dists_trn<dists_val]),\n",
    "        'n_further': np.sum([dists_trn>dists_val]),\n",
    "        'n_equal': np.sum([dists_trn==dists_val]),\n",
    "        'share': [share],\n",
    "        'Avg DCR to Training': [np.mean(dists_trn)],\n",
    "        'Avg DCR to Holdout': [np.mean(dists_val)],\n",
    "    })\n",
    "    return out\n",
    "\n",
    "out = []\n",
    "for dataset in datasets:\n",
    "    trn_fn = f\"{repo}{dataset}_trn.csv.gz\"\n",
    "    trn = pd.read_csv(trn_fn)\n",
    "    print(f\"read {trn_fn} {trn.shape}\")\n",
    "    val_fn = f\"{repo}{dataset}_val.csv.gz\"\n",
    "    val = pd.read_csv(val_fn)\n",
    "    print(f\"read {val_fn} {val.shape}\")\n",
    "    for fn in fns:\n",
    "        syn_fn = f\"{repo}{dataset}_{fn}.csv.gz\"\n",
    "        syn = pd.read_csv(syn_fn)\n",
    "        print(f\"read {syn_fn} {syn.shape}\")\n",
    "        pri = priv(trn, val, syn, is_val=(fn=='val'), c=100)\n",
    "        pri['dataset'] = dataset\n",
    "        pri['synthesizer'] = fn\n",
    "        out.append(pri)\n",
    "            \n",
    "priv = pd.concat(out).sort_values(\"share\", ascending=False)\n",
    "priv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
